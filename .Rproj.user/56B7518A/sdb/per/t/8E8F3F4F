{
    "contents" : "###\n### design of a corpus object\n###\n# (1) texts: a named vector of texts whose only treatment is conversion to unicode\n# (2) attributes: a named list (data-frame) of \"variables\" or chracteristics\n#     of each text\n# (3) attributes labels: an optional user-supplied list of descriptions of each\n#     attribute\n# (4) meta-data: character vector consisting ofL\n#     source (user-supplied or default is full directory path and system)\n#     creation date (automatic)\n#     notes (default is NULL, can be user-supplied)\n\nlibrary(austin)\nif(!require(XML)){\n  print(\"XML package is required for translation\")\n}\nif (!require(RCurl)) {\n  print(\"RCurl package is required for translation\")\n}\n\ntranslateChunk <- function(sourceText, sourceLanguage, targetLanguage, key=NULL, verbose=TRUE) {\n  if (is.null(key)) {\n    key <- \"DQAAALoAAABQS8Lok-tdR8rU1ewhKf1o7IJxS0m_X63cVuDI3ETGyg8rgWhgYTyaXBDdqIe1TUSlCzTbUi70iYQ5bsTOznfk_W9yXNG68iKxExrSxyy5iT5nXbRn3dXONOCcqkNHmJJ-zQAmwP4Gw3uyFEx2A_JES5Xru_Kaq2aJ9hOfRae8h4bqN_PKe7T_HRTg0xhwaNVWGno_tctoe5zXOHcRbEeRyFG-TTrD45ceJMSat7NPF2n7noIPFvL9SNpD026RSPM\"\n  }\n  if (verbose){\n    cat(\"Making call to Google Translate..., with string of length: \", nchar(sourceText), \"\\n\")\n    print(sourceText)\n  }\n  baseUrl <- \"http://translate.google.com/researchapi/translate?\"\n  params <- paste(\"sl=\",sourceLanguage, \"&tl=\", targetLanguage, \"&q=\", sourceText,sep=\"\")\n  \n  url <- paste(baseUrl,params,sep=\"\")\n  header <- paste(\"Authorization: GoogleLogin auth=\", key, sep=\"\")\n  # make the http requst with the url and the authentication header\n  if (verbose) print(url)\n  curl <- getCurlHandle()\n  response <- getURL(url, httpheader=header, curl=curl)\n  # get the http response code to try to see what type of error we're getting\n  code <- getCurlInfo(curl, which=\"response.code\")\n  print(code)\n  rm(curl)\n  Sys.sleep(1)\n  # parse XML response to extract actual translation\n  doc <- xmlTreeParse(response, getDTD = F)\n  r <- xmlRoot(doc) \n  translation <- xmlValue(r[\"entry\"] [[1]] [[5]])\n  return(translation)\n}\n\ntranslate.corpus <- function(corpus, targetlanguageString, \n                             textvar=\"texts\", languagevar=\"language\") {\n  ## function to translate the text from a corpus into another language\n  ## wrapper for translate\n  # initialize the translated text vector\n  translatedTextVector <- rep(NA, nrow(corpus$attribs))\n  for (i in 1:nrow(corpus$attribs)) {\n    if (corpus$attribs[i,textvar]==\"\" | is.na(corpus$attribs[i,textvar])) next\n    if (corpus$attribs[i,languagevar]==targetlanguageString) next\n    translatedTextVector[i] <- translate(corpus$attribs[i,textvar], \n                                         corpus$attribs[i,languagevar],\n                                         targetlanguageString)\n  }\n  return(translatedTextVector)\n}\n\ntranslate <- function(sourceText,  sourceLanguage, targetLanguage, key=NULL, verbose=FALSE){\n  a <- strsplit(sourceText, split=\"[\\\\.]\")\n  sentences <- unlist(a)\n  # Paste sentences together into a chunk until the next one would send the current chunk\n  # over 1000 chars, then send to Google.\n  chunk <- \"\"\n  translatedText <- \"\"\n  for (i in 1:length(sentences)) {\n    s <- sentences[i]\n    if (nchar(s) < 2) {\n      if(verbose) print(\"empty sentence\")\n      next\n    }\n    s <- curlEscape(s)\n    # handle the rare (non-existent?) case of a single sentence being >1000 chars\n    if (nchar(s) >= 1000) {\n      if (verbose) print(\"in the 1000 case\")\n      start <- 1\n      end <- 1000\n      while ((nchar(s) - start) > 1000) {\n        chunk <- substr(s, start, end)\n        translatedText <- paste(translatedText, translateChunk(chunk,sourceLanguage, targetLanguage), sep=\". \")\n        start <- start + 1000\n        end <- end + 1000\n      }\n      chunk <- substr(s, start, nchar(s))\n      translatedText <- paste(translatedText,translateChunk(chunk,sourceLanguage, targetLanguage), sep=\"\")\n      chunk <- \"\"\n    }\n    else {\n      # if this is the last sentence in the speech,\n      # send it and the current chunk (if there is one) to Google\n      if (i==length(sentences)) {\n        if (verbose) print(\"one\")\n        #send to Google, reset the chunk\n        if (nchar(chunk)>5) {\n          translatedText <- paste(translatedText, translateChunk(chunk, sourceLanguage, targetLanguage),sep=\". \")\n        }else{\n          if (verbose) print(\"empty chunk\")\n        }\n        if (nchar(s)>5) {\n          translatedText <- paste(translatedText, translateChunk(s, sourceLanguage, targetLanguage),sep=\". \")\n        } else {\n          if (verbose) print(\"empty sentence\")\n        }\n        chunk <- \"\"\n      }\n      #if this sentence will put the chunk over 1000, send the chunk to Google and save this sentence\n      else if ((nchar(chunk)+nchar(s) >= 1000)) {\n        if (verbose) print(\"two\")\n        translatedText <- paste(translatedText, translateChunk(chunk, sourceLanguage, targetLanguage), sep=\". \")\n        chunk <- paste(s,\".%20\",sep=\"\")\n      } else {\n        if (verbose) print(\"three\")\n        #otherwise just add this sentence to the chunk\n        chunk <- paste(chunk, s, sep=\".%20\")\n      }\n    }\n  }\n  translatedText <- curlUnescape(translatedText)\n  if (verbose) cat(\"****************\", translatedText, \"********************\", nchar(translatedText), \"\\n\")\n  if (verbose) cat(\"\\n\")\n  return(translatedText)\n}\n\n\ngetRootFileNames <- function(longFilenames) {\n  ## function to return just the filename, path not included\n  ## might need to detect .Platform$OS.type to change the delimiter\n  delim <- \"/\"\n  osName <- (Sys.info()[['sysname']] )\n  if(osName==\"Windows\") { delim <- \"\\\\\\\\\" }\n  splitFilenames <- strsplit(longFilenames, delim)\n  return(sapply(splitFilenames, tail, n=1))\n}\n\ngetTextFiles <- function(filenames, textnames=NULL, verbose=FALSE) {\n  # points to files, reads them into a character vector of the texts\n  # with optional names, default being filenames\n  # return a named vector of complete, unedited texts\n  # TODO detect encoding; verbose=TRUE (progress bar?)\n  textsvec <- c()   # initialize text vector\n  # changed from readChar to readLines\n  for (f in filenames) {\n    textsvec = c(textsvec, paste(readLines(file(f)), collapse=\"\\n\")) \n  }\n  # name the vector with the filename by default, otherwise assign \"names\"\n  ifelse(is.null(textnames), \n         names(textsvec) <- getRootFileNames(filenames),\n         names(textsvec) <- textnames)\n  return(textsvec)\n}\n\ngetTextDir <- function(dirname) {\n  # get all files from a directory\n  return(getTextFiles(list.files(dirname, full.names = TRUE)))\n}\n\ngetTextDirGui <- function() {\n  files <- choose.files()\n   #get all files from a directory\n  return(getTextFiles(files))\n}\n\ncorpus.add.attributes <- function(corpus, newattribs, name=newattribs) {\n  # attribs should be a named list of length(corpus$texts)\n  # can be one or more variables\n  newattribs <- as.data.frame(newattribs, stringsAsFactors=FALSE)\n  names(newattribs) <- name\n  corpus$attribs <- cbind(corpus$attribs, newattribs)\n  return(corpus)\n}\n\ncreate.text <- function(string, fname, atts=NULL){\n  # a text has a list of attribute:value pairs\n  if(is.null(atts))\n  {\n    atts <- list(fname=fname)   \n  }\n  temp.text <- list(string=string, atts=atts)\n  class(temp.text) <- list(\"text\", class(temp.text))\n  return(temp.text)\n}\n\ncorpus.create <- function(texts, textnames=NULL, attribs=NULL, source=NULL, notes=NULL, attribs.labels=NULL) {\n  if (is.null(names(texts))) \n    names(texts) <- paste(\"text\", 1:length(texts), sep=\"\")\n  if (is.null(source)) \n    source <- paste(getwd(), \"/* \", \"on \",  Sys.info()[\"machine\"], \" by \", Sys.info()[\"user\"], sep=\"\")\n  created <- date()\n  metadata <- c(source=source, created=created, notes=notes)\n  if (!is.null(attribs)) {\n    attribs <- data.frame(texts=texts,\n                          attribs,\n                          row.names=names(texts), \n                          check.rows=TRUE, stringsAsFactors=FALSE)\n  }\n  if (!is.null(attribs) & is.null(attribs.labels)) {\n    attribs.labels <- c(\"Original texts\", rep(NULL, length(attribs)-1))\n  }\n  temp.corpus <- list(attribs=attribs,\n                      attribs.labels=attribs.labels,\n                      metadata=metadata)\n  class(temp.corpus) <- list(\"corpus\", class(temp.corpus))\n  return(temp.corpus)\n}\n\ncorpus.append <- function(corpus1, newtexts, newattribs, ...) {\n  # function to add new texts and attributes to an existing corpus\n  # should make it also allow an optional corpus2 version where two\n  # corpuses could be combined with corpus.append(corp1, corp2)\n  # if we can verify the same attribute set.\n  tempcorpus <- corpus.create(newtexts, attribs=newattribs)\n  corpus1$attribs <- rbind(tempcorpus$attribs, corpus1$attribs)\n  # TODO: implement concatenation of any attribs.labels from new corpus\n  return(corpus1)\n}\n\nsummary.corpus <- function(corpus, texts=\"texts\", subset=NULL, select=NULL, drop=FALSE, output=TRUE, nmax=100) {\n  corpus <- corpus.subset.inner(corpus, substitute(subset), substitute(select))\n  cat(\"Corpus object contains\", nrow(corpus$attribs), \"texts.\\n\\n\")\n  # allow user to set the column or variable which identifies the texts to summarize\n  texts <- corpus$attribs[,texts]\n  attribs <- as.data.frame(corpus$attribs[,-1])\n  #print(as.character(substitute(select))[2])\n  if (ncol(attribs)==1) names(attribs) <- as.character(substitute(select))[2]\n  #print(names(attribs))\n  names(texts) <- rownames(corpus$attribs)\n  print(head(cbind((dtexts <- describetexts(texts, output=FALSE)),\n                   attribs), \n             nmax))\n  cat(\"\\nSource:  \", corpus$metadata[\"source\"], \".\\n\", sep=\"\")\n  cat(\"Created: \", corpus$metadata[\"created\"], \".\\n\", sep=\"\")\n  cat(\"Notes:   \", corpus$metadata[\"notes\"], \".\\n\\n\", sep=\"\")\n  # invisibly pass the summary of the texts from describetexts()\n  return(invisible(dtexts))\n}\n\ndescribetexts <- function(texts, output=TRUE) {\n  # need to implement subsetting here too\n  string <- gsub(\"[[:punct:][:digit:]]\", \"\", texts)\n  string <- gsub(\"\\n\", \"\", string)\n  string <- string[string!=\"\"]\n  string <- tolower(string)\n  tokenized.string <- lapply(string, function(s) strsplit(s, c(\" \", \".\", \"?\", \"!\")))\n  ntokens <- sapply(tokenized.string, function(s) sapply(s, length))\n  ntypes  <- sapply(tokenized.string, function(s) sapply(s, function(s2) length(unique(s2))))\n  nsents  <- sapply(texts, function(s) length(gregexpr(\"[.!?]\", s)[[1]]))\n  if (output) {\n    cat(\"Summary of texts:\\n\", substitute(texts), sep=\"\")\n    print(data.frame(Texts=names(texts),\n                     Types=ntypes,\n                     Tokens=ntokens,\n                     Sentences=nsents,\n                     row.names=NULL))\n  }\n  return(invisible(list(ntokens=ntokens, ntypes=ntypes, nsents=nsents)))\n}\n\ntokenize <- function(input.text, input.text.name=\"count\") {\n  # returns a dataframe of word counts, word is 1st column\n  #\n  ## clean up stuff in the text\n  clean.txt <- gsub(\"[[:punct:][:digit:]]\", \"\", input.text)\n  # for French, make \"l'\" into \"l\"\n  input.text <- gsub(\"l'\", \"l \", input.text)\n  # make all \"Eszett\" characters in Hochdeutsche into \"ss\" as in Swiss German\n  clean.txt <- gsub(\"ß\", \"ss\", clean.txt)\n  # make all words lowercase\n  clean.txt <- tolower(clean.txt)\n  # tokenize\n  tokenized.txt <- scan(what=\"char\", text=clean.txt, quiet=TRUE)\n  # flush out \"empty\" strings caused by removal of punctuation and numbers\n  tokenized.txt <- tokenized.txt[tokenized.txt!=\"\"]\n  ## tabulate word counts\n  ## and return as a data frame with variables \"word\" and given name\n  wf.list <- as.data.frame(table(tokenized.txt))\n  names(wf.list) <- c(\"feature\", input.text.name)\n  return(wf.list)\n}\n\n\ncreate.fvm.corpus <- function(corpus,\n                              feature=c(\"word\"),\n                              groups=NULL,\n                              subset=NULL, \n                              verbose=TRUE) {\n  # default is to take word as features, could expand that\n    \n  if (verbose) cat(\"Creating fvm:\\n\")\n  \n  # new subset feature (no \"select\" because inappropriate here)\n  corpus <- corpus.subset.inner(corpus, substitute(subset))\n  \n  # new aggregation - to concatenate texts before making the fvm \n  if (!is.null(groups)) {\n    if (verbose) cat(\"  Now aggregating by group: \", groups, \"...\", sep=\"\") \n    texts <- split(corpus$attribs$texts, as.factor(corpus$attribs[,groups]))\n    texts <- sapply(texts, paste)\n    if (verbose) cat(\"complete.\\n\")\n  } else {\n     texts <- corpus$attribs$texts\n     names(texts) <- rownames(corpus$attribs)\n  }\n    \n  textnames <- names(texts)\n  #save(texts, file=\"temptexts\")\n  fvm <- data.frame(feature=NA)\n  progress.threshold <- .1\n  if (verbose) cat(\"  Progress (%): [0\")\n  for (i in 1:length(texts)) {\n    if (i/length(texts) > progress.threshold) {\n      if (verbose) cat(paste(\"...\", progress.threshold*100, sep=\"\"))\n      progress.threshold <- progress.threshold+.1\n    }\n    temp.fvm <- tokenize(texts[i], textnames[i])\n    fvm <- merge(fvm, temp.fvm, by=\"feature\", all=TRUE)\n  }\n  # convert NAs to zeros\n  fvm[is.na(fvm)] <- 0\n  # drop any words that never occur\n  fvm <- fvm[-which(apply(fvm[,-1], 1, sum)==0),]\n  if (verbose) cat(\"]\\n\")\n  # make rownames the feature name, and remove the feature name as a data column\n  rownames(fvm) <- fvm[,\"feature\"]\n  fvm <- fvm[,-1]\n  \n  if (verbose) cat(\"\\n\")\n  return(fvm)\n}\n\n\n##\n## Ken's ongoing efforts to get the bleedin' subsetting function working!!!!\n## AND SUCCESS!!!! YEAAAAHHH!!!!\ncorpus.subset.inner <- function(corpus, subsetExpr=NULL, selectExpr=NULL, drop=FALSE) {\n  # This is the \"inner\" function to be called by other functions\n  # to return a subset directly, use corpus.subset\n  \n  # The select argument exists only for the methods for data frames and matrices. \n  # It works by first replacing column names in the selection expression with the \n  # corresponding column numbers in the data frame and then using the resulting \n  # integer vector to index the columns. This allows the use of the standard indexing \n  # conventions so that for example ranges of columns can be specified easily, \n  # or single columns can be dropped\n  # as in:\n  # subset(airquality, Temp > 80, select = c(Ozone, Temp))\n  # subset(airquality, Day == 1, select = -Temp)\n  # subset(airquality, select = Ozone:Wind)\n  if (is.null(subsetExpr)) \n    rows <- TRUE\n  else {\n    rows <- eval(subsetExpr, corpus$attribs, parent.frame())\n    if (!is.logical(rows)) \n      stop(\"'subset' must evaluate to logical\")\n    rows <- rows & !is.na(rows)\n  }\n  if (is.null(selectExpr)) \n    vars <- TRUE\n  else {\n    nl <- as.list(seq_along(corpus$attribs))\n    names(nl) <- names(corpus$attribs)\n    vars <- c(1, eval(selectExpr, nl, parent.frame()))\n  }\n  # implement subset, select, and drop\n  corpus$attribs <- corpus$attribs[rows, vars, drop=drop]\n  return(corpus)\n}\n\ncorpus.subset <- function(corpus, subset=NULL, select=NULL) {\n  tempcorp <- corpus.subset.inner(corpus, substitute(subset), substitute(select))\n  return(tempcorp)\n}",
    "created" : 1345027461242.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "167392557",
    "id" : "8E8F3F4F",
    "lastKnownWriteTime" : 1344942410,
    "path" : "~/Dropbox/QUANTESS/QUANTEDA/QUANTEDA_0.11.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}