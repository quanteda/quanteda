% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenizers.R
\name{customized_tokenizer}
\alias{customized_tokenizer}
\title{Customized ICU tokenizers}
\usage{
customized_tokenizer(
  base = c("ICU_word", "word", "sentence", "none"),
  split_hyphens = FALSE,
  split_tags = FALSE,
  custom_rules = ""
)
}
\arguments{
\item{base}{the base rules for the ICU RBBI}

\item{split_hyphens}{only relevant if \code{base = "ICU_word"}. Define the split
(or not) of hyphenated words in the customized tokenizer. Override the
behaviour of \code{tokens()}.}

\item{split_tags}{only relevant if \code{base = "ICU_word"}. Define the split (or
not) of hashtags (#) and usernames (@). Override the behaviour of
\code{tokens()}.}

\item{custom_rules}{a character of length one specifying rules to be appended
at the end of the base rules.}
}
\value{
a function usable as the \code{"what"} argument of the \code{tokens()}
function.
}
\description{
Helper defining a custom quanteda/stringi tokenizer by defining a set of
rules for the ICU Rule-based Break Iterator (RBBI).
}
\details{
The base rules were obtained from
\href{https://github.com/unicode-org/icu/tree/main/icu4c/source/data/brkitr/rules}{icu/icu4c/source/data/brkitr/rules/},
slightly modified for compatibility with quanteda. The \code{"word"} rule is
equivalent to the regular \code{what = "word"} tokenization of the \code{tokens()}
function. In contrast, \code{"ICU_word"} is the real baseline for the word rules
and skips some internals of quanteda normally used to retain special
character, related for example to URLs and hyphens.

Note that the \code{"sentence"} base does not include additional rules to
prevent splits caused by abbreviations such as "Mr.". Consider importing as
well the \href{https://github.com/unicode-org/icu/tree/main/icu4c/source/data/brkitr}{language-specific abbreviations}.
}
\section{Resources about the Rule-based Break Iterator}{

\url{https://unicode-org.github.io/icu/userguide/boundaryanalysis/break-rules.html}
\url{http://sujitpal.blogspot.com/2008/05/tokenizing-text-with-icu4js.html}
}

\examples{
txt <- c(doc = "I've been sick today, I may go to the hospital.",
         doc_fr = "J'ai été malade aujourd'hui, je vais aller à l'hôpital.")
tokens(txt, what = customized_tokenizer())

## Implement custom elision rule for french
Elision_french <- "
$Elision = ([lLmMtTnNsSjJdDcC]|([jJ][u][s]|[qQ][u][o][i]|[lL][o][r][s]|[pP][u][i][s])?[qQ][u])[\u0027\u2019];
# Disable chaining so it only matches beginning of word.
^$Elision / $ALetterPlus;
"

tokens(txt, what = customized_tokenizer(custom_rules = Elision_french))
}
\seealso{
tokens
}
\keyword{tokens}
