% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/corpus_chunk.R
\name{corpus_chunk}
\alias{corpus_chunk}
\title{Segment a corpus into chunks of a given size}
\usage{
corpus_chunk(
  x,
  size,
  inflation_factor = 1,
  truncate = FALSE,
  use_docvars = TRUE,
  verbose = quanteda_options("verbose")
)
}
\arguments{
\item{size}{integer; the (approximate) token length of the chunks. See
Details.}

\item{inflation_factor}{numeric; the number of single default quanteda tokens
required to produce a single chunked token. This is designed for chunking
for input to LLMs, since LLM tokenizers (such as LLaMA's SentencePiece
Byte-Pair Encoding tokenizer) require more tokens than the linguistically
defined grammatically-based tokenizer that is the \pkg{quanteda} default.
The recommended setting for this is 0.8, meaning that if size is 100, then
the actual chunk size will be 100 / 0.8 = 125. Defaults to 1.0.}

\item{truncate}{logical; if \code{TRUE}, truncate the text after \code{size}}
}
\description{
Segment a corpus into new documents of roughly equal sized text chunks, with
the possibility of overlapping the chunks.
}
\details{
The token length is estimated using \code{stringi::stri_length(txt) / stringi::stri_count_boundaries(txt)} to avoid needing to tokenize and rejoin
the corpus from the tokens. Combined with \code{inflation_factor}, this means
that the exact token length for chunking will be approximate.
}
\examples{
data_corpus_inaugural[1] |>
  corpus_chunk(size = 10)

}
\seealso{
\code{\link[=tokens_chunk]{tokens_chunk()}}
}
\keyword{corpus}
