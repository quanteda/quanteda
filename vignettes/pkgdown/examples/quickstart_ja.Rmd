---
title: "クイック・スタートガイド"
output: 
  html_document:
    toc: true
    css: ja.css
---

```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = FALSE, 
                      comment = "##")
```

この記事では，**quanteda**の特徴と機能の概要を説明します．より詳しい説明は，[ quanteda.ioにある記事](http://docs.quanteda.io/articles/index.html)を参照してください．

# パッケージのインストール

**quanteda**は[CRAN](https://CRAN.R-project.org/package=quanteda)からインストールできます．GUIのRパッケージインストーラを使用してインストールするか，次のコマンドを実行します．

```{r, eval = FALSE}
install.packages("quanteda") 
```

GitHubから最新の開発バージョンをインストールする方法については，https://github.com/kbenoit/quanteda を参照してください．

## インストールが推奨されるパッケージ

**quanteda**には連携して機能を拡張する一連のパッケージがあり，それらをインストールすることをお薦めします．

* [**readtext**](https://github.com/kbenoit/readtext)：多くの入力形式からテキストデータをRに簡単に読み込むパッケージ
* [**spacyr**](https://github.com/kbenoit/spacyr)：Pythonの[spaCy](http://spacy.io)ライブラリを使用した自然言語解析のためのパッケージで，品詞タグ付け，固有表現抽出，および係り受け関係の解析などができる
* [**quantedaData**](https://github.com/kbenoit/quantedaData)：**quanteda**の本記事内の説明で使用する追加のテキストデータ

    ```{r eval = FALSE}
    devtools::install_github("kbenoit/quantedaData")
    ```
*  [**LIWCalike**](https://github.com/kbenoit/LIWCalike): [Linguistic Inquiry and Word Count](http://liwc.wpengine.com) (LIWC) アプローチによるテキスト分析のR実装
    ```{r eval = FALSE}
    devtools::install_github("kbenoit/LIWCalike")
    ```

# コーパスの作成

まず，**quanteda**を読み込んで，パッケージの関数とデータにアクセスできるようにします．

```{r, message = FALSE}
library(quanteda)
```

## 利用可能なコーパス

**quanteda**にはテキストを読み込むためのシンプルで強力なパッケージ，[**readtext**](https://github.com/kbenoit/readtext)があります．このパッケージの`readtext()`は，ローカス・ストレージやインターネットからファイル読み込み，`corpus()`にデータ・フレームを返します．

`readtext()`で利用可能なファイルやデータの形式:

* テキスト（`.txt`）ファイル
* コンマ区切り値（`.csv`）ファイル
* XML形式のデータ
* JSON形式のFacebook APIのデータ
* JSON形式のTwitter APIのデータ
* 一般的なJSONデータ

**quanteda**のコーパスを生成する関数である `corpus()`は，以下の種類のデータを読み込むことができます．

* 文字列ベクトル（例：**readtext**以外のツールを使用して読み込んだテキスト）
* **tm**パッケージの `VCorpus`コーパスオブジェクト
* テキスト列と他の文書に対応したメタデータを含むデータ・フレーム

### 文字列からコーパスを作成

コーパスを作成する最も簡単な方法は，`corpus()`を用いて，すでにRに読み込まれた文字列ベクトル作成することです．文字列ベクトルをRに取り込む方法はさまざまなので，高度なRユーザーは，コーパスをいろいろな方法で作り出せます．

次の例では，**quanteda**パッケージに含まれているイギリスの政党が2010年の総選挙のために発行したマニフェストのテキストデータ（`data_char_ukimmig2010`）からコーパスを作成しています．

```{r}
myCorpus <- corpus(data_char_ukimmig2010)  # テキストからコーパスを作成
summary(myCorpus)
```

コーパスを作成したあとでも，`docvars`を用いると，必要に応じて文書に対応した変数をこのコーパスに追加することができます．

たとえば，Rの`names()`関数を使って文字ベクトル（`data_char_ukimmig2010`）の名前を取得し，これを文書変数（`docvar()`）に追加することができます．

```{r}
docvars(myCorpus, "Party") <- names(data_char_ukimmig2010)
docvars(myCorpus, "Year") <- 2010
summary(myCorpus)
```

分析の対象となる文書変数ではではないけれども，文書の属性として残しておきたいと思うメタデータも，`docvars()`を使って，コーパスに追加することができます．

```{r}
metadoc(myCorpus, "language") <- "english"
metadoc(myCorpus, "docsource")  <- paste("data_char_ukimmig2010", 1:ndoc(myCorpus), sep = "_")
summary(myCorpus, showmeta = TRUE)
```

`metadoc()`を用いると，文書メタデータのフィールドを自由に定義することができますが，単一の値（"english"）を文書変数（"language"）に付与するときには，Rが値を繰り替えして全ての文書に同じ値を付与していることに注意してください．

独自の文書メタデータのフィールド（`docsource`）を作成するために，**quanteda**の関数である`ndoc()`を使ってコーパスに含まれる文書の総数を取得しています．`ndoc()`は，`nrow()`や`ncol()`などのRの標準の関数と同じような方法で動作するように設計されています．

### readtextパッケージを用いたファイルの読み込み

```{r, eval=FALSE}
require(readtext)

# Twitter json
mytf1 <- readtext("~/Dropbox/QUANTESS/social media/zombies/tweets.json")
myCorpusTwitter <- corpus(mytf1)
summary(myCorpusTwitter, 5)
# generic json - needs a textfield specifier
mytf2 <- readtext("~/Dropbox/QUANTESS/Manuscripts/collocations/Corpora/sotu/sotu.json",
                  textfield = "text")
summary(corpus(mytf2), 5)
# text file
mytf3 <- readtext("~/Dropbox/QUANTESS/corpora/project_gutenberg/pg2701.txt", cache = FALSE)
summary(corpus(mytf3), 5)
# multiple text files
mytf4 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", cache = FALSE)
summary(corpus(mytf4), 5)
# multiple text files with docvars from filenames
mytf5 <- readtext("~/Dropbox/QUANTESS/corpora/inaugural/*.txt", 
                  docvarsfrom = "filenames", sep = "-", docvarnames = c("Year", "President"))
summary(corpus(mytf5), 5)
# XML data
mytf6 <- readtext("~/Dropbox/QUANTESS/quanteda_working_files/xmlData/plant_catalog.xml", 
                  textfield = "COMMON")
summary(corpus(mytf6), 5)
# csv file
write.csv(data.frame(inaugSpeech = texts(data_corpus_inaugural), 
                     docvars(data_corpus_inaugural)),
          file = "/tmp/inaug_texts.csv", row.names = FALSE)
mytf7 <- readtext("/tmp/inaug_texts.csv", textfield = "inaugSpeech")
summary(corpus(mytf7), 5)
```

## コーパスオブジェクトの使い方

### コーパスの原理

**quanteda**のコーパスは，元の文書をユニコード（UTF-8）に変換し，文書に対するメタデータと一緒に格納しすることで、ステミングや句読点の削除などの処理よって変更されないテキストデータの静的な保管庫になるように設計されています．これによって，コーパスから文書を抽出して新しいオブジェクトを作成した後でも，コーパスには元のデータが残り，別の分析を，同じコーパスを用いて行うことができます．

コーパスから文書を取り出すためには，`texts()`と呼ばれる関数を使用します．

```{r}
texts(data_corpus_inaugural)[2]
```


`summary()`により，コーパス内のテキストの要約を行うことができます．

```{r}
summary(data_corpus_irishbudget2010)
```

`summary()`の出力をデータ・フレームとして保存し，基本的な記述統計を描画することができます．

```{r, fig.width = 8}
tokenInfo <- summary(data_corpus_inaugural)
if (require(ggplot2))
    ggplot(data=tokenInfo, aes(x = Year, y = Tokens, group = 1)) + geom_line() + geom_point() +
        scale_x_continuous(labels = c(seq(1789,2012,12)), breaks = seq(1789,2012,12) ) 

# Longest inaugural address: William Henry Harrison
tokenInfo[which.max(tokenInfo$Tokens), ] 
```


## コーパスに対する操作

### コーパスの結合

`+`演算子を用いると，簡単に二個のコーパスを連結できます．コーパスが異なる構造を持つ場合でも，文書変数が失われることはなく，コーパスのメタデータも引き継がれます．

```{r}
library(quanteda)
mycorpus1 <- corpus(data_corpus_inaugural[1:5])
mycorpus2 <- corpus(data_corpus_inaugural[53:58])
mycorpus3 <- mycorpus1 + mycorpus2
summary(mycorpus3)
```

### コーパス内の文書の一部の文書の抽出

`corpus_subset()`により，文書変数に適用される論理条件に基づいて文書を抽出することができます．

```{r}
summary(corpus_subset(data_corpus_inaugural, Year > 1990))
summary(corpus_subset(data_corpus_inaugural, President == "Adams"))
```

## コーパス内の文書の探索

`kwic()`（keywords-in-context）は単語の検索を行い，その単語が現れる文脈を表示します．

```{r, tidy=TRUE}
kwic(data_corpus_inaugural, "terror")
```

```{r}
kwic(data_corpus_inaugural, "terror", valuetype = "regex")
```

```{r}
kwic(data_corpus_inaugural, "communist*")
```


上記の要約では，"Year"と"President"は各文書に結び付けられた変数です． `docvars()`でこのような変数にアクセスできます．

```{r}
# inspect the document-level variables
head(docvars(data_corpus_inaugural))

# inspect the corpus-level metadata
metacorpus(data_corpus_inaugural)
```

[**quantedaData**](http://github.com/kbenoit/quantedaData)をインストールすることで，より多くのコーパスを試すことができます．


# コーパスから特長を抽出

文書のスケーリングなどの統計分析を行うためには，それぞれの文書の特長をまとめた行列を作成する必要があります． **quanteda**では，このような行列を生成するために `dfm()`を使います． dfmは*document-feature  matrix*の略で，行が文書（document），列が特長（feature）となる行列です．行と列をこのように定義する理由は，データ分析では行が分析単位になり，各列が分析対象になる変数となるのが一般的だからです．多くのソフトウェアでは，この行列を*document-term matrix*と呼びます．**quanteda**が語（term）でなはく特長（feature）という用語を使うのは，特長のほうが一般性を持つ用語だからです．テキスト分析では，単語，語幹，単語の集合，Nグラム，品詞など様々なものが文書の特長となります．

## 文書のトークン化

テキストを簡単にトークン化するために，**quanteda**は `tokens()`と呼ばれる強力なコマンドを提供します．この関数は，文字ベクトルのトークンのリストからなる中間オブジェクトを生成します．ここで，リストの一つ一つの要素は入力された文書に対応しています．

`tokens()`は意図して保守的に設計されており、ユーザーが明示的に指示を与えないかぎりは，要素を削除しません．

```{r}
txt <- c(text1 = "This is $10 in 999 different ways,\n up and down; left and right!", 
         text2 = "@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123.")
tokens(txt)
tokens(txt, remove_numbers = TRUE,  remove_punct = TRUE)
tokens(txt, remove_numbers = FALSE, remove_punct = TRUE)
tokens(txt, remove_numbers = TRUE,  remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE)
tokens(txt, remove_numbers = FALSE, remove_punct = FALSE, remove_separators = FALSE)
```

`tokens`には個々の文字をトークン化するオプションもあります．
```{r}
tokens("Great website: http://textasdata.com?page=123.", what = "character")
tokens("Great website: http://textasdata.com?page=123.", what = "character", 
         remove_separators = FALSE)
```

もしくは，一文ごとにトークン化するオプションもあります．
```{r}
# sentence level         
tokens(c("Kurt Vongeut said; only assholes use semi-colons.", 
         "Today is Thursday in Canberra:  It is yesterday in London.", 
         "En el caso de que no puedas ir con ellos, ¿quieres ir con nosotros?"), 
         what = "sentence")
```

## 文書行列の作成

文書をトークン化は中間的な処理であり，ほとんどのユーザーはこれを省いて、すぐに文書行列を作成したいと考えるでしょう． このために，**quanteda**は，自動的にトークン化を実行し，文書行列を作成する`dfm()`と呼ばれるスイスアーミーナイフのように便利な関数を持っています．保守的な`tokens()`とは異なり，`dfm()`はデフォルトで大文字から小文字への置換や，句読点を除去などの操作を適用します．また，`dfm()`から`tokens()`の全てのオプションを利用できます．

```{r}
myCorpus <- corpus_subset(data_corpus_inaugural, Year > 1990)

# make a dfm
myDfm <- dfm(myCorpus)
myDfm[, 1:5]
```

`dfm()`の追加のオプションには，ストップワードの削除（`remove`）や語のステミング（`stem`）が含まれます．

```{r}
# make a dfm, removing stopwords and applying stemming
myStemMat <- dfm(myCorpus, remove = stopwords("english"), 
                 stem = TRUE, remove_punct = TRUE)
myStemMat[, 1:5]
```

`remove`によっては，文書行列から除外するトークンを指定します．`stopwords()`は，幾つかの言語で定義されたストップワードのリストを返します（残念ながら日本語は含まれていませんが，ユーザーが定義した辞書を使うことはできます）．

```{r}
head(stopwords("english"), 20)
head(stopwords("russian"), 10)
head(stopwords("arabic"), 10)
```

### 文書行列の表示

RStudioの"Environment"パネル，またはRの`View()`を用いることで，dfmに格納された値を見ることができます．dfmに対して`plot()`を用いると，[**wordcloud**](https://cran.r-project.org/web/packages/wordcloud)を使ってワードクラウドが表示されます．

```{r warning=FALSE, fig.width = 8, fig.height = 8}
mydfm <- dfm(data_char_ukimmig2010, remove = stopwords("english"), remove_punct = TRUE)
mydfm
```

頻度が最も高い特長を見るには，`topfeatures()`を使います．

```{r}
topfeatures(mydfm, 20)  # 20 top words
```

dfmを`textplot_wordcloud()`に渡すことで，ワードクラウドを描画できます．この関数は，オブジェクトや引数を**wordcloud**パッケージの`wordcloud()`に送るので，ワードクラウドの表示を変更することもできます．

```{r warning=FALSE, fig.width = 7, fig.height = 7}
set.seed(100)
textplot_wordcloud(mydfm, min.freq = 6, random.order = FALSE,
                   rot.per = .25, 
                   colors = RColorBrewer::brewer.pal(8,"Dark2"))
```

### 変数による文書のグループ化

**quanteda**では，dfmを作成する際に，文書変数の値によって文書をグループ化するすることができます．

```{r}
byPartyDfm <- dfm(data_corpus_irishbudget2010, groups = "party", 
                  remove = stopwords("english"), remove_punct = TRUE)
```

また、以下のように，dfmを語の頻度順に並べ替えて，中身を確かめられます．

```{r}
dfm_sort(byPartyDfm)[, 1:10]
```


### 辞書による語のグループ化

肯定的な映画のレビューや，特定の政治意識に関する語があらかじめ分かっている場合，語のグループをまとめて一つのものとして取り扱い，これらを合計して単一の特長として分析すると上手くいくかもしれません．

次の例では，テロリズムに関連する言葉や経済に関連する言葉が，クリントン以降の大統領演説コーパスでどのように異なるかを見てみます．

```{r}
recentCorpus <- corpus_subset(data_corpus_inaugural, Year > 1991)
```

テロリズムと経済という2つのリストからなる辞書を作成します．

```{r}
myDict <- dictionary(list(terror = c("terrorism", "terrorists", "threat"),
                          economy = c("jobs", "business", "grow", "work")))
```


文書行列を作成するときに，この辞書を`dfm()`の`dictionary`に渡します．

```{r}
byPresMat <- dfm(recentCorpus, dictionary = myDict)
byPresMat
```

`dictionary()`は，LIWCやWordstatなどの一般的な辞書ファイルを読み込むことができますです．以下では，LIWCの辞書を大統領就任演説コーパスに適用しています．

```{r, eval = FALSE}
liwcdict <- dictionary(file = "~/Dropbox/QUANTESS/dictionaries/LIWC/LIWC2001_English.dic",
                       format = "LIWC")
liwcdfm <- dfm(data_corpus_inaugural[52:58], dictionary = liwcdict)
liwcdfm[, 1:10]
```


# 追加の事例

## 文書の類似性

```{r fig.width = 6}
presDfm <- dfm(corpus_subset(data_corpus_inaugural, Year > 1980), 
               remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
obamaSimil <- textstat_simil(presDfm, c("2009-Obama" , "2013-Obama"), 
                             margin = "documents", method = "cosine")
obamaSimil
# dotchart(as.list(obamaSimil)$"2009-Obama", xlab = "Cosine similarity")
```


上記の文書間の類似性から樹形図を作成して，大統領を分類することができます．
```{r, fig.width = 10, fig.height = 7, eval = FALSE}
data(data_corpus_SOTU, package = "quantedaData")
presDfm <- dfm(corpus_subset(data_corpus_SOTU, Date > as.Date("1980-01-01")), 
               stem = TRUE, remove_punct = TRUE,
               remove = stopwords("english"))
presDfm <- dfm_trim(presDfm, min_count = 5, min_docfreq = 3)
# hierarchical clustering - get distances on normalized dfm
presDistMat <- textstat_dist(dfm_weight(presDfm, "relfreq"))
# hiarchical clustering the distance object
presCluster <- hclust(presDistMat)
# label with document names
presCluster$labels <- docnames(presDfm)
# plot as a dendrogram
plot(presCluster, xlab = "", sub = "", main = "Euclidean Distance on Normalized Token Frequency")
```

文書間と同様に用語間の類似性も測定できます．
```{r}
sim <- textstat_simil(presDfm, c("fair", "health", "terror"), method = "cosine", margin = "features")
lapply(as.list(sim), head, 10)
```

## 文書のスケーリング

**quanteda**にはたくさんの計量テキスト分析のためのモデルが含まれていますが，ワードフィッシュ（`textmodel_wordfish()`）による教師なしの文書のスケーリングを使ってみます．

```{r}
# make prettier document names
ieDfm <- dfm(data_corpus_irishbudget2010)
textmodel_wordfish(ieDfm, dir = c(2, 1))
```

## トピックモデル

`convert()`を用いると，dfmを**topicmodels**の`LDA()`形式のデータに転換して，簡単にトピックモデルを適用できます．

```{r}
quantdfm <- dfm(data_corpus_irishbudget2010, 
                remove_punct = TRUE, remove_numbers = TRUE, remove = stopwords("english"))
quantdfm <- dfm_trim(quantdfm, min_count = 4, max_docfreq = 10)
quantdfm

if (require(topicmodels)) {
    myLDAfit20 <- LDA(convert(quantdfm, to = "topicmodels"), k = 20)
    get_terms(myLDAfit20, 5)
}
```
