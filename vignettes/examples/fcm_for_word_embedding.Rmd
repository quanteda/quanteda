---
title: "Example: fcm for word embedding "
author: Kenneth Benoit, Haiyan Wang and Kohei Watanabe
output: 
  rmarkdown::html_vignette:
    toc: no
---
```{r, echo = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "##")
```

```{r, message = FALSE}
library(quanteda)
library(text2vec)
```

Unsupervisedly learned word embeddings have seen tremendous success in numerous NLP tasks in recent years.This vignette provides a basic overview on how to apply **quanteda**'s feature co-occurrence matrix for **GloVe** word embedding.  

###Read the corpus 
from the [**text2vec** vignette](http://text2vec.org/glove.html):
```{r eval = TRUE}
temp_file <- '/tmp/wiki.RDS'
if (!file.exists(temp_file)) {
    wiki <- corpus(readtext::readtext("http://mattmahoney.net/dc/text8.zip", verbosity = 0))
    saveRDS(wiki, file = paste0(temp_file))
} else {
    wiki <- readRDS(paste0(temp_file))
}
```

##Trim the features before constructing the fcm:
```{r}
wiki_dfm <- dfm(wiki, verbose = TRUE)
feat <- featnames(dfm_trim(wiki_dfm, min_count = 5, verbose = TRUE))
wiki_toks <- tokens(wiki) %>% tokens_select(feat, padding = TRUE)
```

##Constructing the feature co-occurrence matrix(fcm):
```{r}
wiki_fcm <- fcm(wiki_toks, context = "window", count = "weighted", 
                weights = 1/(1:5), tri = TRUE)
```

## Fit the [GloVe model](https://nlp.stanford.edu/pubs/glove.pdf) using package [text2vec](http://text2vec.org)

GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.

Glove encodes the ratios of word-word co-occurrence probabilities, which is thought to represent some crude form of meaning associated with the abstract concept of the word, as vector difference. The training objective of GloVe is to learn word vectors such that their dot product equals the logarithm of the words' probability of co-occurrence.  
```{r}
glove = GlobalVectors$new(word_vectors_size = 50, vocabulary = types(wiki_toks), x_max = 10)
wiki_main = fit_transform(wiki_fcm, glove, n_iter = 20)
```

##The Glove model leans two sets of word vectors 
The two vectors are main and context. According to Glove paper, averaging the two word vectors results in more accurate representation.
```{r}
wiki_context = glove$components
dim(wiki_context)

wiki_vectors = wiki_main + t(wiki_context)
```

## Now we can find the closest word vectors for `paris - france + germany`
```{r}
berlin <-  wiki_vectors["paris", , drop = FALSE] - 
  wiki_vectors["france", , drop = FALSE] + 
  wiki_vectors["germany", , drop = FALSE]

# calculate the similarity
wiki_vector_dfm <- as.dfm(rbind(wiki_vectors, berlin))
wiki_vector_dfm@Dimnames$docs[dim(wiki_vector_dfm)[1]] <- "new_berlin"
cos_sim <-  textstat_simil(wiki_vector_dfm, "new_berlin", 
                           margin = "documents", method= "cosine")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```

## Here is another example for `london = paris - france + uk + england`
```{r}
london <-  wiki_vectors["paris", , drop = FALSE] - 
    wiki_vectors["france", , drop = FALSE] + 
    wiki_vectors["uk", , drop = FALSE] + 
    wiki_vectors["england", , drop = FALSE] 

wiki_vector_dfm <- as.dfm(rbind(wiki_vectors, london))
wiki_vector_dfm@Dimnames$docs[dim(wiki_vector_dfm)[1]] <- "new_london"
cos_sim <-  textstat_simil(wiki_vector_dfm, "new_london", 
                           margin = "documents", method= "cosine")
head(sort(cos_sim[,1], decreasing = TRUE), 5)
```
