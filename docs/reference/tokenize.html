<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>tokenize a set of texts — tokenize • quanteda</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>

<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">quanteda</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>tokenize a set of texts</h1>
    </div>

    
    <p>Tokenize the texts from a character vector or from a corpus.

    <code>is.tokenizedTexts</code> returns <code>TRUE</code> if the object is of class tokenizedTexts, <code>FALSE</code> otherwise.</p>
    

    <pre class="usage"><span class='fu'>tokenize</span>(<span class='no'>x</span>, <span class='no'>...</span>)

<span class='co'># S3 method for character</span>
<span class='fu'>tokenize</span>(<span class='no'>x</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"word"</span>, <span class='st'>"sentence"</span>, <span class='st'>"character"</span>,
  <span class='st'>"fastestword"</span>, <span class='st'>"fasterword"</span>), <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_symbols</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_separators</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>,
  <span class='kw'>remove_twitter</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_hyphens</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>ngrams</span> <span class='kw'>=</span> <span class='fl'>1L</span>, <span class='kw'>skip</span> <span class='kw'>=</span> <span class='fl'>0L</span>, <span class='kw'>concatenator</span> <span class='kw'>=</span> <span class='st'>"_"</span>, <span class='kw'>simplify</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>,
  <span class='kw'>verbose</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='no'>...</span>)

<span class='co'># S3 method for corpus</span>
<span class='fu'>tokenize</span>(<span class='no'>x</span>, <span class='no'>...</span>)

<span class='fu'>is.tokenizedTexts</span>(<span class='no'>x</span>)

<span class='fu'>as.tokenizedTexts</span>(<span class='no'>x</span>, <span class='no'>...</span>)

<span class='co'># S3 method for list</span>
<span class='fu'>as.tokenizedTexts</span>(<span class='no'>x</span>, <span class='no'>...</span>)

<span class='co'># S3 method for tokens</span>
<span class='fu'>as.tokenizedTexts</span>(<span class='no'>x</span>, <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>text(s) or corpus to be tokenized</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>additional arguments not used</p></td>
    </tr>
    <tr>
      <th>what</th>
      <td><p>the unit for splitting the text, available alternatives are:</p><dl class='dl-horizontal'>
<dt><code>"word"</code></dt><dd><p>(recommended default) smartest, but 
slowest, word tokenization method; see 
<a href='http://www.rdocumentation.org/packages/stringi/topics/stringi-search-boundaries'>stringi-search-boundaries</a> for details.</p></dd> 
<dt><code>"fasterword"</code></dt><dd><p>dumber, but faster, word tokenizeation method, 
uses <a href='http://www.rdocumentation.org/packages/stringi/topics/stri_split'>stri_split_charclass</a>(x, "\\pWHITE_SPACE")</p></dd> 
<dt><code>"fastestword"</code></dt><dd><p>dumbest, but fastest, word tokenization method,
calls <code><a href='http://www.rdocumentation.org/packages/stringi/topics/stri_split'>stri_split_fixed</a>(x, " ")</code></p></dd> 
<dt><code>"character"</code></dt><dd><p>tokenization into individual characters</p></dd> 
<dt><code>"sentence"</code></dt><dd><p>sentence segmenter, smart enough to handle some 
exceptions in English such as "Prof. Plum killed Mrs. Peacock." (but far 
from perfect).</p></dd> </dl></td>
    </tr>
    <tr>
      <th>remove_numbers</th>
      <td><p>remove tokens that consist only of numbers, but not 
words that start with digits, e.g. <code>2day</code></p></td>
    </tr>
    <tr>
      <th>remove_punct</th>
      <td><p>if <code>TRUE</code>, remove all characters in the Unicode 
"Punctuation" [P] class</p></td>
    </tr>
    <tr>
      <th>remove_symbols</th>
      <td><p>if <code>TRUE</code>, remove all characters in the Unicode 
"Symbol" [S] class</p></td>
    </tr>
    <tr>
      <th>remove_separators</th>
      <td><p>remove Separators and separator characters (spaces 
and variations of spaces, plus tab, newlines, and anything else in the 
Unicode "separator" category) when <code>remove_punct=FALSE</code>.  Only 
applicable for <code>what = "character"</code> (when you probably want it to be 
<code>FALSE</code>) and for <code>what = "word"</code> (when you probably want it to be
<code>TRUE</code>).  Note that if <code>what = "word"</code> and you set 
<code>remove_punct = TRUE</code>, then <code>remove_separators</code> has no effect.  Use
carefully.</p></td>
    </tr>
    <tr>
      <th>remove_twitter</th>
      <td><p>remove Twitter characters <code>@</code> and <code>#</code>; set to
<code>TRUE</code> if you wish to eliminate these.</p></td>
    </tr>
    <tr>
      <th>remove_hyphens</th>
      <td><p>if <code>TRUE</code>, split words that are connected by 
hyphenation and hyphenation-like characters in between words, e.g. 
<code>"self-storage"</code> becomes <code>c("self", "storage")</code>.  Default is 
<code>FALSE</code> to preserve such words as is, with the hyphens.  Only applies 
if <code>what = "word"</code>.</p></td>
    </tr>
    <tr>
      <th>remove_url</th>
      <td><p>if <code>TRUE</code>, find and eliminate URLs beginning with
http(s) -- see section "Dealing with URLs".</p></td>
    </tr>
    <tr>
      <th>ngrams</th>
      <td><p>integer vector of the <em>n</em> for <em>n</em>-grams, defaulting 
to <code>1</code> (unigrams). For bigrams, for instance, use <code>2</code>; for 
bigrams and unigrams, use <code>1:2</code>.  You can even include irregular 
sequences such as <code>2:3</code> for bigrams and trigrams only.  See 
<code><a href='tokens_ngrams.html'>tokens_ngrams</a></code>.</p></td>
    </tr>
    <tr>
      <th>skip</th>
      <td><p>integer vector specifying the skips for skip-grams, default is 0 
for only immediately neighbouring words. Only applies if <code>ngrams</code> is 
different from the default of 1.  See <code><a href='ngrams.html'>skipgrams</a></code>.</p></td>
    </tr>
    <tr>
      <th>concatenator</th>
      <td><p>character to use in concatenating <em>n</em>-grams, default
is "<code>_</code>", which is recommended since this is included in the regular 
expression and Unicode definitions of "word" characters</p></td>
    </tr>
    <tr>
      <th>simplify</th>
      <td><p>if <code>TRUE</code>, return a character vector of tokens rather 
than a list of length <code><a href='ndoc.html'>ndoc</a>(texts)</code>, with each element of the 
list containing a character vector of the tokens corresponding to that 
text.</p></td>
    </tr>
    <tr>
      <th>verbose</th>
      <td><p>if <code>TRUE</code>, print timing messages to the console; off by 
default</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>A list of length <code><a href='ndoc.html'>ndoc</a>(x)</code> of the tokens found in each text.</p>
<p>a <strong>tokenizedText</strong> (S3) object, essentially a list of character
  vectors. If <code>simplify = TRUE</code> then return a single character vector.</p>
    
    <h2 class="hasAnchor" id="details"><a class="anchor" href="#details"></a>Details</h2>

    <p>The tokenizer is designed to be fast and flexible as well as to 
  handle Unicode correctly. Most of the time, users will construct <a href='dfm.html'>dfm</a>
  objects from texts or a corpus, without calling <code>tokenize()</code> as an 
  intermediate step.  Since <code>tokenize()</code> is most likely to be used by 
  more technical users, we have set its options to default to minimal 
  intervention. This means that punctuation is tokenized as well, and that 
  nothing is removed by default from the text being tokenized except 
  inter-word spacing and equivalent characters.
    <code>as.tokenizedTexts</code> coerces a list of character tokens to a tokenizedText class object, 
making the methods available for this object type available to this object.
    <code>as.tokenizedTexts</code> coerces tokenizedTextsHashed to a
  tokenizedText class object, making the methods available for this object
  type available to this object.</p>
    
    <h2 class="hasAnchor" id="dealing-with-urls"><a class="anchor" href="#dealing-with-urls"></a>Dealing with URLs</h2>

    <p>URLs are tricky to tokenize, because they contain
  a number of symbols and punctuation characters.  If you wish to remove 
  these, as most people do, and your text contains URLs, then you should set
  <code>what = "fasterword"</code> and <code>remove_url = TRUE</code>.  If you wish to
  keep the URLs, but do not want them mangled, then your options are more
  limited, since removing punctuation and symbols will also remove them from
  URLs.  We are working on improving this behaviour.
    See the examples below.</p>
    
    <h2 class="hasAnchor" id="see-also"><a class="anchor" href="#see-also"></a>See also</h2>

    <p><code><a href='tokens_ngrams.html'>tokens_ngrams</a></code></p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'># same for character vectors and for lists</span>
<span class='no'>tokensFromChar</span> <span class='kw'>&lt;-</span> <span class='fu'>tokenize</span>(<span class='no'>data_corpus_inaugural</span>[<span class='fl'>1</span>:<span class='fl'>3</span>])
<span class='no'>tokensFromCorp</span> <span class='kw'>&lt;-</span> <span class='fu'>tokenize</span>(<span class='fu'><a href='corpus_subset.html'>corpus_subset</a></span>(<span class='no'>data_corpus_inaugural</span>, <span class='no'>Year</span><span class='kw'>&lt;</span><span class='fl'>1798</span>))
<span class='fu'>identical</span>(<span class='no'>tokensFromChar</span>, <span class='no'>tokensFromCorp</span>)</div><div class='output co'>#&gt; [1] TRUE</div><div class='input'><span class='fu'>str</span>(<span class='no'>tokensFromChar</span>)</div><div class='output co'>#&gt; List of 3
#&gt;  $ 1789-Washington: chr [1:1540] "Fellow" "-" "Citizens" "of" ...
#&gt;  $ 1793-Washington: chr [1:147] "Fellow" "citizens" "," "I" ...
#&gt;  $ 1797-Adams     : chr [1:2584] "When" "it" "was" "first" ...
#&gt;  - attr(*, "class")= chr [1:2] "tokenizedTexts" "list"
#&gt;  - attr(*, "what")= chr "word"
#&gt;  - attr(*, "ngrams")= int 1
#&gt;  - attr(*, "concatenator")= chr ""</div><div class='input'><span class='co'># returned as a list</span>
<span class='fu'>head</span>(<span class='fu'>tokenize</span>(<span class='no'>data_corpus_inaugural</span>[<span class='fl'>57</span>])<span class='kw'>[[</span><span class='fl'>1</span>]], <span class='fl'>10</span>)</div><div class='output co'>#&gt;  [1] "Vice"      "President" "Biden"     ","         "Mr"        "."        
#&gt;  [7] "Chief"     "Justice"   ","         "Members"  </div><div class='input'><span class='co'># returned as a character vector using simplify=TRUE</span>
<span class='fu'>head</span>(<span class='fu'>tokenize</span>(<span class='no'>data_corpus_inaugural</span>[<span class='fl'>57</span>], <span class='kw'>simplify</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>), <span class='fl'>10</span>)</div><div class='output co'>#&gt;  [1] "Vice"      "President" "Biden"     ","         "Mr"        "."        
#&gt;  [7] "Chief"     "Justice"   ","         "Members"  </div><div class='input'>
<span class='co'># removing punctuation marks and lowecasing texts</span>
<span class='fu'>head</span>(<span class='fu'>tokenize</span>(<span class='fu'><a href='char_tolower.html'>char_tolower</a></span>(<span class='no'>data_corpus_inaugural</span>[<span class='fl'>57</span>]), <span class='kw'>simplify</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>), <span class='fl'>30</span>)</div><div class='output co'>#&gt;  [1] "vice"          "president"     "biden"         "mr"           
#&gt;  [5] "chief"         "justice"       "members"       "of"           
#&gt;  [9] "the"           "united"        "states"        "congress"     
#&gt; [13] "distinguished" "guests"        "and"           "fellow"       
#&gt; [17] "citizens"      "each"          "time"          "we"           
#&gt; [21] "gather"        "to"            "inaugurate"    "a"            
#&gt; [25] "president"     "we"            "bear"          "witness"      
#&gt; [29] "to"            "the"          </div><div class='input'><span class='co'># keeping case and punctuation</span>
<span class='fu'>head</span>(<span class='fu'>tokenize</span>(<span class='no'>data_corpus_inaugural</span>[<span class='fl'>57</span>], <span class='kw'>simplify</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>), <span class='fl'>30</span>)</div><div class='output co'>#&gt;  [1] "Vice"          "President"     "Biden"         ","            
#&gt;  [5] "Mr"            "."             "Chief"         "Justice"      
#&gt;  [9] ","             "Members"       "of"            "the"          
#&gt; [13] "United"        "States"        "Congress"      ","            
#&gt; [17] "distinguished" "guests"        ","             "and"          
#&gt; [21] "fellow"        "citizens"      ":"             "Each"         
#&gt; [25] "time"          "we"            "gather"        "to"           
#&gt; [29] "inaugurate"    "a"            </div><div class='input'><span class='co'># keeping versus removing hyphens</span>
<span class='fu'>tokenize</span>(<span class='st'>"quanteda data objects are auto-loading."</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "quanteda"     "data"         "objects"      "are"          "auto-loading"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='st'>"quanteda data objects are auto-loading."</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_hyphens</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "quanteda" "data"     "objects"  "are"      "auto"     "loading" 
#&gt; </div><div class='input'><span class='co'># keeping versus removing symbols</span>
<span class='fu'>tokenize</span>(<span class='st'>"&lt;tags&gt; and other + symbols."</span>, <span class='kw'>remove_symbols</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "&lt;"       "tags"    "&gt;"       "and"     "other"   "+"       "symbols"
#&gt; [8] "."      
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='st'>"&lt;tags&gt; and other + symbols."</span>, <span class='kw'>remove_symbols</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "tags"    "and"     "other"   "symbols"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='st'>"&lt;tags&gt; and other + symbols."</span>, <span class='kw'>remove_symbols</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"fasterword"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "&lt;tags&gt;"   "and"      "other"    "+"        "symbols."
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='st'>"&lt;tags&gt; and other + symbols."</span>, <span class='kw'>remove_symbols</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"fasterword"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "tags"     "and"      "other"    "symbols."
#&gt; </div><div class='input'>
<span class='co'>## examples with URLs - hardly perfect!</span>
<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='st'>"Repo https://githib.com/kbenoit/quanteda, and www.stackoverflow.com."</span>
<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "Repo"                  "and"                   "www.stackoverflow.com"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "Repo"                  "https"                 "githib.com"           
#&gt; [4] "kbenoit"               "quanteda"              "and"                  
#&gt; [7] "www.stackoverflow.com"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"fasterword"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "Repo"                          "httpsgithibcomkbenoitquanteda"
#&gt; [3] "and"                           "wwwstackoverflowcom"          
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"fasterword"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "Repo"                                
#&gt; [2] "https://githib.com/kbenoit/quanteda,"
#&gt; [3] "and"                                 
#&gt; [4] "www.stackoverflow.com."              
#&gt; </div><div class='input'>

<span class='co'>## MORE COMPARISONS</span>
<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='st'>"#textanalysis is MY &lt;3 4U @myhandle gr8 #stuff :-)"</span>
<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "#textanalysis" "is"            "MY"            "3"            
#&gt; [5] "4U"            "@myhandle"     "gr8"           "#stuff"       
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_twitter</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt; [1] "textanalysis" "is"           "MY"           "3"            "4U"          
#&gt; [6] "myhandle"     "gr8"          "stuff"       
#&gt; </div><div class='input'><span class='co'>#tokenize("great website http://textasdata.com", remove_url = FALSE)</span>
<span class='co'>#tokenize("great website http://textasdata.com", remove_url = TRUE)</span>

<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='fu'>c</span>(<span class='kw'>text1</span><span class='kw'>=</span><span class='st'>"This is $10 in 999 different ways,\n up and down; left and right!"</span>,
         <span class='kw'>text2</span><span class='kw'>=</span><span class='st'>"@kenbenoit working: on #quanteda 2day\t4ever, http://textasdata.com?page=123."</span>)
<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>verbose</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; <span class='message'>Starting tokenization...</span></div><div class='output co'>#&gt; <span class='message'>  ...preserving Twitter characters (#, @)</span></div><div class='output co'>#&gt; <span class='message'>...total elapsed: 0 seconds.</span></div><div class='output co'>#&gt; <span class='message'>  ...tokenizing texts</span></div><div class='output co'>#&gt; <span class='message'></span>
#&gt; <span class='message'>  ...removing separators.</span></div><div class='output co'>#&gt; <span class='message'>...total elapsed:  0 seconds.</span></div><div class='output co'>#&gt; <span class='message'>  ...replacing Twitter characters (#, @)</span></div><div class='output co'>#&gt; <span class='message'>...total elapsed: 0 seconds.</span></div><div class='output co'>#&gt; <span class='message'>  ...replacing names</span></div><div class='output co'>#&gt; <span class='message'>...total elapsed:  0.000999999999990564 seconds.</span></div><div class='output co'>#&gt; <span class='message'>Finished tokenizing and cleaning 2 texts.</span></div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "$"         "10"        "in"        "999"      
#&gt;  [7] "different" "ways"      ","         "up"        "and"       "down"     
#&gt; [13] ";"         "left"      "and"       "right"     "!"        
#&gt; 
#&gt; text2 :
#&gt;  [1] "@kenbenoit"     "working"        ":"              "on"            
#&gt;  [5] "#quanteda"      "2day"           "4ever"          ","             
#&gt;  [9] "http"           ":"              "/"              "/"             
#&gt; [13] "textasdata.com" "?"              "page"           "="             
#&gt; [17] "123"            "."             
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "in"        "different" "ways"      "up"       
#&gt;  [7] "and"       "down"      "left"      "and"       "right"    
#&gt; 
#&gt; text2 :
#&gt; [1] "@kenbenoit"     "working"        "on"             "#quanteda"     
#&gt; [5] "2day"           "4ever"          "http"           "textasdata.com"
#&gt; [9] "page"          
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "10"        "in"        "999"       "different"
#&gt;  [7] "ways"      "up"        "and"       "down"      "left"      "and"      
#&gt; [13] "right"    
#&gt; 
#&gt; text2 :
#&gt;  [1] "@kenbenoit"     "working"        "on"             "#quanteda"     
#&gt;  [5] "2day"           "4ever"          "http"           "textasdata.com"
#&gt;  [9] "page"           "123"           
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "$"         "in"        "different" "ways"     
#&gt;  [7] ","         "up"        "and"       "down"      ";"         "left"     
#&gt; [13] "and"       "right"     "!"        
#&gt; 
#&gt; text2 :
#&gt;  [1] "@kenbenoit"     "working"        ":"              "on"            
#&gt;  [5] "#quanteda"      "2day"           "4ever"          ","             
#&gt;  [9] "http"           ":"              "/"              "/"             
#&gt; [13] "textasdata.com" "?"              "page"           "="             
#&gt; [17] "."             
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "$"         "10"        "in"        "999"      
#&gt;  [7] "different" "ways"      ","         "up"        "and"       "down"     
#&gt; [13] ";"         "left"      "and"       "right"     "!"        
#&gt; 
#&gt; text2 :
#&gt;  [1] "@kenbenoit"     "working"        ":"              "on"            
#&gt;  [5] "#quanteda"      "2day"           "4ever"          ","             
#&gt;  [9] "http"           ":"              "/"              "/"             
#&gt; [13] "textasdata.com" "?"              "page"           "="             
#&gt; [17] "123"            "."             
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>, <span class='kw'>remove_separators</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      " "         "is"        " "         "$"         "10"       
#&gt;  [7] " "         "in"        " "         "999"       " "         "different"
#&gt; [13] " "         "ways"      ","         "\n"        " "         "up"       
#&gt; [19] " "         "and"       " "         "down"      ";"         " "        
#&gt; [25] "left"      " "         "and"       " "         "right"     "!"        
#&gt; 
#&gt; text2 :
#&gt;  [1] "@kenbenoit"     " "              "working"        ":"             
#&gt;  [5] " "              "on"             " "              "#quanteda"     
#&gt;  [9] " "              "2day"           "\t"             "4ever"         
#&gt; [13] ","              " "              "http"           ":"             
#&gt; [17] "/"              "/"              "textasdata.com" "?"             
#&gt; [21] "page"           "="              "123"            "."             
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_numbers</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>remove_url</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 2 documents.
#&gt; text1 :
#&gt;  [1] "This"      "is"        "in"        "different" "ways"      "up"       
#&gt;  [7] "and"       "down"      "left"      "and"       "right"    
#&gt; 
#&gt; text2 :
#&gt; [1] "@kenbenoit" "working"    "on"         "#quanteda"  "2day"      
#&gt; [6] "4ever"     
#&gt; </div><div class='input'>
<span class='co'># character level</span>
<span class='fu'>tokenize</span>(<span class='st'>"Great website: http://textasdata.com?page=123."</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"character"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt;  [1] "G" "r" "e" "a" "t" "w" "e" "b" "s" "i" "t" "e" ":" "h" "t" "t" "p" ":" "/"
#&gt; [20] "/" "t" "e" "x" "t" "a" "s" "d" "a" "t" "a" "." "c" "o" "m" "?" "p" "a" "g"
#&gt; [39] "e" "=" "1" "2" "3" "."
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='st'>"Great website: http://textasdata.com?page=123."</span>, <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"character"</span>,
         <span class='kw'>remove_separators</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 1 document.
#&gt; Component 1 :
#&gt;  [1] "G" "r" "e" "a" "t" " " "w" "e" "b" "s" "i" "t" "e" ":" " " "h" "t" "t" "p"
#&gt; [20] ":" "/" "/" "t" "e" "x" "t" "a" "s" "d" "a" "t" "a" "." "c" "o" "m" "?" "p"
#&gt; [39] "a" "g" "e" "=" "1" "2" "3" "."
#&gt; </div><div class='input'>
<span class='co'># sentence level         </span>
<span class='fu'>tokenize</span>(<span class='fu'>c</span>(<span class='st'>"Kurt Vongeut said; only assholes use semi-colons."</span>,
           <span class='st'>"Today is Thursday in Canberra:  It is yesterday in London."</span>,
           <span class='st'>"Today is Thursday in Canberra:  \nIt is yesterday in London."</span>,
           <span class='st'>"To be?  Or\nnot to be?"</span>),
          <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"sentence"</span>)</div><div class='output co'>#&gt; tokenizedTexts from 4 documents.
#&gt; Component 1 :
#&gt; [1] "Kurt Vongeut said; only assholes use semi-colons."
#&gt; 
#&gt; Component 2 :
#&gt; [1] "Today is Thursday in Canberra:  It is yesterday in London."
#&gt; 
#&gt; Component 3 :
#&gt; [1] "Today is Thursday in Canberra:   It is yesterday in London."
#&gt; 
#&gt; Component 4 :
#&gt; [1] "To be?"        "Or not to be?"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>data_corpus_inaugural</span>[<span class='fu'>c</span>(<span class='fl'>2</span>,<span class='fl'>40</span>)], <span class='kw'>what</span> <span class='kw'>=</span> <span class='st'>"sentence"</span>, <span class='kw'>simplify</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt;  [1] "Fellow citizens, I am again called upon by the voice of my country to execute the functions of its Chief Magistrate."                                                                                                                                                                                                                                       
#&gt;  [2] "When the occasion proper for it shall arrive, I shall endeavor to express the high sense I entertain of this distinguished honor, and of the confidence which has been reposed in me by the people of united America."                                                                                                                                      
#&gt;  [3] "Previous to the execution of any official act of the President the Constitution requires an oath of office."                                                                                                                                                                                                                                                
#&gt;  [4] "This oath I am now about to take, and in your presence: That if it shall be found during my administration of the Government I have in any instance violated willingly or knowingly the injunctions thereof, I may (besides incurring constitutional punishment) be subject to the upbraidings of all who are now witnesses of the present solemn ceremony."
#&gt;  [5] "Chief Justice, Mr. Vice President, my friends, you will understand and, I believe, agree with my wish that the form of this inauguration be simple and its words brief."                                                                                                                                                                                    
#&gt;  [6] "We Americans of today, together with our allies, are passing through a period of supreme test."                                                                                                                                                                                                                                                             
#&gt;  [7] "It is a test of our courage -- of our resolve -- of our wisdom -- our essential democracy."                                                                                                                                                                                                                                                                 
#&gt;  [8] "If we meet that test -- successfully and honorably -- we shall perform a service of historic importance which men and women and children will honor throughout all time."                                                                                                                                                                                   
#&gt;  [9] "As I stand here today, having taken the solemn oath of office in the presence of my fellow countrymen -- in the presence of our God -- I know that it is America's purpose that we shall not fail."                                                                                                                                                         
#&gt; [10] "In the days and in the years that are to come we shall work for a just and honorable peace, a durable peace, as today we work and fight for total victory in war."                                                                                                                                                                                          
#&gt; [11] "We can and we will achieve such a peace."                                                                                                                                                                                                                                                                                                                   
#&gt; [12] "We shall strive for perfection."                                                                                                                                                                                                                                                                                                                            
#&gt; [13] "We shall not achieve it immediately -- but we still shall strive."                                                                                                                                                                                                                                                                                          
#&gt; [14] "We may make mistakes -- but they must never be mistakes which result from faintness of heart or abandonment of moral principle."                                                                                                                                                                                                                            
#&gt; [15] "I remember that my old schoolmaster, Dr. Peabody, said, in days that seemed to us then to be secure and untroubled: \"Things in life will not always run smoothly."                                                                                                                                                                                         
#&gt; [16] "Sometimes we will be rising toward the heights -- then all will seem to reverse itself and start downward."                                                                                                                                                                                                                                                 
#&gt; [17] "The great fact to remember is that the trend of civilization itself is forever upward; that a line drawn through the middle of the peaks and the valleys of the centuries always has an upward trend.\""                                                                                                                                                    
#&gt; [18] "Our Constitution of 1787 was not a perfect instrument; it is not perfect yet."                                                                                                                                                                                                                                                                              
#&gt; [19] "But it provided a firm base upon which all manner of men, of all races and colors and creeds, could build our solid structure of democracy."                                                                                                                                                                                                                
#&gt; [20] "And so today, in this year of war, 1945, we have learned lessons -- at a fearful cost -- and we shall profit by them."                                                                                                                                                                                                                                      
#&gt; [21] "We have learned that we cannot live alone, at peace; that our own well-being is dependent on the well-being of other nations far away."                                                                                                                                                                                                                     
#&gt; [22] "We have learned that we must live as men, not as ostriches, nor as dogs in the manger."                                                                                                                                                                                                                                                                     
#&gt; [23] "We have learned to be citizens of the world, members of the human community."                                                                                                                                                                                                                                                                               
#&gt; [24] "We have learned the simple truth, as Emerson said, that \"The only way to have a friend is to be one.\""                                                                                                                                                                                                                                                    
#&gt; [25] "We can gain no lasting peace if we approach it with suspicion and mistrust or with fear."                                                                                                                                                                                                                                                                   
#&gt; [26] "We can gain it only if we proceed with the understanding, the confidence, and the courage which flow from conviction."                                                                                                                                                                                                                                      
#&gt; [27] "The Almighty God has blessed our land in many ways."                                                                                                                                                                                                                                                                                                        
#&gt; [28] "He has given our people stout hearts and strong arms with which to strike mighty blows for freedom and truth."                                                                                                                                                                                                                                              
#&gt; [29] "He has given to our country a faith which has become the hope of all peoples in an anguished world."                                                                                                                                                                                                                                                        
#&gt; [30] "So we pray to Him now for the vision to see our way clearly -- to see the way that leads to a better life for ourselves and for all our fellow men -- to the achievement of His will to peace on earth."                                                                                                                                                    </div><div class='input'>
<span class='co'># removing features (stopwords) from tokenized texts</span>
<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='char_tolower.html'>char_tolower</a></span>(<span class='fu'>c</span>(<span class='kw'>mytext1</span> <span class='kw'>=</span> <span class='st'>"This is a short test sentence."</span>,
                      <span class='kw'>mytext2</span> <span class='kw'>=</span> <span class='st'>"Short."</span>,
                      <span class='kw'>mytext3</span> <span class='kw'>=</span> <span class='st'>"Short, shorter, and shortest."</span>))
<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt; [1] "this"     "is"       "a"        "short"    "test"     "sentence"
#&gt; 
#&gt; mytext2 :
#&gt; [1] "short"
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short"    "shorter"  "and"      "shortest"
#&gt; </div><div class='input'><span class='fu'><a href='removeFeatures.html'>removeFeatures</a></span>(<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>), <span class='fu'><a href='stopwords.html'>stopwords</a></span>(<span class='st'>"english"</span>))</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt; [1] "short"    "test"     "sentence"
#&gt; 
#&gt; mytext2 :
#&gt; [1] "short"
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short"    "shorter"  "shortest"
#&gt; </div><div class='input'>
<span class='co'># ngram tokenization</span>
<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>ngrams</span> <span class='kw'>=</span> <span class='fl'>2</span>)</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt; [1] "this_is"       "is_a"          "a_short"       "short_test"   
#&gt; [5] "test_sentence"
#&gt; 
#&gt; mytext2 :
#&gt; character(0)
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short_shorter" "shorter_and"   "and_shortest" 
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>ngrams</span> <span class='kw'>=</span> <span class='fl'>2</span>, <span class='kw'>skip</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>concatenator</span> <span class='kw'>=</span> <span class='st'>" "</span>)</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt; [1] "this a"         "is short"       "a test"         "short sentence"
#&gt; 
#&gt; mytext2 :
#&gt; character(0)
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short and"        "shorter shortest"
#&gt; </div><div class='input'><span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>ngrams</span> <span class='kw'>=</span> <span class='fl'>1</span>:<span class='fl'>2</span>)</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt;  [1] "this"          "is"            "a"             "short"        
#&gt;  [5] "test"          "sentence"      "this_is"       "is_a"         
#&gt;  [9] "a_short"       "short_test"    "test_sentence"
#&gt; 
#&gt; mytext2 :
#&gt; [1] "short"
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short"         "shorter"       "and"           "shortest"     
#&gt; [5] "short_shorter" "shorter_and"   "and_shortest" 
#&gt; </div><div class='input'><span class='co'># removing features from ngram tokens</span>
<span class='fu'><a href='removeFeatures.html'>removeFeatures</a></span>(<span class='fu'>tokenize</span>(<span class='no'>txt</span>, <span class='kw'>remove_punct</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>, <span class='kw'>ngrams</span> <span class='kw'>=</span> <span class='fl'>1</span>:<span class='fl'>2</span>), <span class='fu'><a href='stopwords.html'>stopwords</a></span>(<span class='st'>"english"</span>))</div><div class='output co'>#&gt; tokenizedTexts from 3 documents.
#&gt; mytext1 :
#&gt; [1] "short"         "test"          "sentence"      "this_is"      
#&gt; [5] "is_a"          "a_short"       "short_test"    "test_sentence"
#&gt; 
#&gt; mytext2 :
#&gt; [1] "short"
#&gt; 
#&gt; mytext3 :
#&gt; [1] "short"         "shorter"       "shortest"      "short_shorter"
#&gt; [5] "shorter_and"   "and_shortest" 
#&gt; </div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#details">Details</a></li>

      <li><a href="#dealing-with-urls">Dealing with URLs</a></li>

      <li><a href="#see-also">See also</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

    <h2>Author</h2>
    
Ken Benoit and Paul Nulty

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Kenneth Benoit.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
