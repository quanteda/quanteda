<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Naive Bayes classifier for texts — textmodel_NB • quanteda</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->

<link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  
  
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">quanteda</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="..//index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/pkgdown_only/design.html">quanteda Structure and Design</a>
    </li>
    <li>
      <a href="../articles/pkgdown_only/LitVignette.html">Digital Humanities Use Case: Replication of analyses from *Text Analysis with R for Students of Literature*</a>
    </li>
    <li>
      <a href="../articles/pkgdown_only/plotting.html">Plotting with quanteda</a>
    </li>
    <li>
      <a href="../articles/quickstart.html">Getting Started with quanteda</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">News</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Naive Bayes classifier for texts</h1>
    </div>

    
    <p>Fit a multinomial or Bernoulli Naive Bayes model, given a dfm and some
training labels.</p>
    

    <pre class="usage"><span class='fu'>textmodel_NB</span>(<span class='no'>x</span>, <span class='no'>y</span>, <span class='kw'>smooth</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"uniform"</span>, <span class='st'>"docfreq"</span>, <span class='st'>"termfreq"</span>),
  <span class='kw'>distribution</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"multinomial"</span>, <span class='st'>"Bernoulli"</span>), <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>the <a href='dfm.html'>dfm</a> on which the model will be fit.  Does not need to contain 
only the training documents.</p></td>
    </tr>
    <tr>
      <th>y</th>
      <td><p>vector of training labels associated with each document identified 
in <code>train</code>.  (These will be converted to factors if not already 
factors.)</p></td>
    </tr>
    <tr>
      <th>smooth</th>
      <td><p>smoothing parameter for feature counts by class</p></td>
    </tr>
    <tr>
      <th>prior</th>
      <td><p>prior distribution on texts; see Details</p></td>
    </tr>
    <tr>
      <th>distribution</th>
      <td><p>count model for text features, can be <code>multinomial</code> 
or <code>Bernoulli</code>.  To fit a "binary multinomial" model, first convert the 
dfm to a binary matrix using <code><a href='tf.html'>tf</a>(x, "boolean")</code>.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>more arguments passed through</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>A list of return values, consisting of:</p>
<dt>call</dt><dd><p>original function call</p></dd>

<dt>PwGc</dt><dd><p>probability of the word given the class (empirical 
  likelihood)</p></dd>

<dt>Pc</dt><dd><p>class prior probability</p></dd>

<dt>PcGw</dt><dd><p>posterior class probability given the word</p></dd>

<dt>Pw</dt><dd><p>baseline probability of the word</p></dd>

<dt>data</dt><dd><p>list consisting of <code>x</code> training class, and <code>y</code> 
  test class</p></dd>

<dt>distribution</dt><dd><p>the distribution argument</p></dd>

<dt>prior</dt><dd><p>argument passed as a prior</p></dd>

<dt>smooth</dt><dd><p>smoothing parameter</p></dd>

    
    <h2 class="hasAnchor" id="predict-methods"><a class="anchor" href="#predict-methods"></a>Predict Methods</h2>

    <p>A <code>predict</code> method is also available for a 
  fitted Naive Bayes object, see <code><a href='predict.textmodel.html'>predict.textmodel_NB_fitted</a></code>.</p>
    
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). Introduction
  to Information Retrieval. Cambridge University Press.
  <a href='https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf'>https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a></p>  
<p>Jurafsky, Daniel and James H. Martin. (2016) <em>Speech and Language Processing.</em>  Draft of November 7, 2016.
  <a href='https://web.stanford.edu/~jurafsky/slp3/6.pdf'>https://web.stanford.edu/~jurafsky/slp3/6.pdf</a></p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'>## Example from 13.1 of _An Introduction to Information Retrieval_</span>
<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='fu'>c</span>(<span class='kw'>d1</span> <span class='kw'>=</span> <span class='st'>"Chinese Beijing Chinese"</span>,
         <span class='kw'>d2</span> <span class='kw'>=</span> <span class='st'>"Chinese Chinese Shanghai"</span>,
         <span class='kw'>d3</span> <span class='kw'>=</span> <span class='st'>"Chinese Macao"</span>,
         <span class='kw'>d4</span> <span class='kw'>=</span> <span class='st'>"Tokyo Japan Chinese"</span>,
         <span class='kw'>d5</span> <span class='kw'>=</span> <span class='st'>"Chinese Chinese Chinese Tokyo Japan"</span>)
<span class='no'>trainingset</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='dfm.html'>dfm</a></span>(<span class='no'>txt</span>, <span class='kw'>tolower</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)
<span class='no'>trainingclass</span> <span class='kw'>&lt;-</span> <span class='fu'>factor</span>(<span class='fu'>c</span>(<span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"N"</span>, <span class='fl'>NA</span>), <span class='kw'>ordered</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)

<span class='co'>## replicate IIR p261 prediction for test set (document 5)</span>
(<span class='no'>nb.p261</span> <span class='kw'>&lt;-</span> <span class='fu'>textmodel_NB</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"docfreq"</span>))</div><div class='output co'>#&gt; Fitted Naive Bayes model:
#&gt; Call:
#&gt; 	textmodel_NB.dfm(x = trainingset, y = trainingclass, prior = "docfreq")
#&gt; 
#&gt; 
#&gt; Training classes and priors:
#&gt;    Y    N 
#&gt; 0.75 0.25 
#&gt; 
#&gt; 		  Likelihoods:		Class Posteriors:
#&gt; 6 x 4 Matrix of class "dgeMatrix"
#&gt;                   Y         N         Y         N
#&gt; Chinese  0.42857143 0.2222222 0.8526316 0.1473684
#&gt; Beijing  0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Shanghai 0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Macao    0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Tokyo    0.07142857 0.2222222 0.4909091 0.5090909
#&gt; Japan    0.07142857 0.2222222 0.4909091 0.5090909
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='no'>nb.p261</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>trainingset</span>[<span class='fl'>5</span>, ])</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;       lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d5 -8.10769 -8.906681    0.6898 0.3102         Y
#&gt; </div><div class='input'>
<span class='co'># contrast with other priors</span>
<span class='fu'>predict</span>(<span class='fu'>textmodel_NB</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"uniform"</span>))</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d1 -4.333653 -5.898527    0.8271 0.1729         Y
#&gt; d2 -4.333653 -5.898527    0.8271 0.1729         Y
#&gt; d3 -3.486355 -4.394449    0.7126 0.2874         Y
#&gt; d4 -6.818560 -5.205379    0.1661 0.8339         N
#&gt; d5 -8.513155 -8.213534    0.4257 0.5743         N
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='fu'>textmodel_NB</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"termfreq"</span>))</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d1 -3.958960 -6.504662    0.9273 0.0727         Y
#&gt; d2 -3.958960 -6.504662    0.9273 0.0727         Y
#&gt; d3 -3.111662 -5.000585    0.8686 0.1314         Y
#&gt; d4 -6.443866 -5.811515    0.3470 0.6530         N
#&gt; d5 -8.138462 -8.819670    0.6640 0.3360         Y
#&gt; </div><div class='input'>
<span class='co'>## replicate IIR p264 Bernoulli Naive Bayes</span>
(<span class='no'>nb.p261.bern</span> <span class='kw'>&lt;-</span> <span class='fu'>textmodel_NB</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>distribution</span> <span class='kw'>=</span> <span class='st'>"Bernoulli"</span>,
                              <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"docfreq"</span>))</div><div class='output co'>#&gt; Fitted Naive Bayes model:
#&gt; Call:
#&gt; 	textmodel_NB.dfm(x = trainingset, y = trainingclass, prior = "docfreq", 
#&gt;     distribution = "Bernoulli")
#&gt; 
#&gt; 
#&gt; Training classes and priors:
#&gt;    Y    N 
#&gt; 0.75 0.25 
#&gt; 
#&gt; 		  Likelihoods:		Class Posteriors:
#&gt; 6 x 4 Matrix of class "dgeMatrix"
#&gt;            Y         N         Y         N
#&gt; Chinese  0.8 0.6666667 0.7826087 0.2173913
#&gt; Beijing  0.4 0.3333333 0.7826087 0.2173913
#&gt; Shanghai 0.4 0.3333333 0.7826087 0.2173913
#&gt; Macao    0.4 0.3333333 0.7826087 0.2173913
#&gt; Tokyo    0.2 0.6666667 0.4736842 0.5263158
#&gt; Japan    0.2 0.6666667 0.4736842 0.5263158
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='no'>nb.p261.bern</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>trainingset</span>[<span class='fl'>5</span>, ])</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d5 -5.262178 -3.819085    0.1911 0.8089         N
#&gt; </div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#predict-methods">Predict Methods</a></li>

      <li><a href="#references">References</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

    <h2>Author</h2>
    
Kenneth Benoit

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Kenneth Benoit.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
