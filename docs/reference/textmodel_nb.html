<!-- Generated by pkgdown: do not edit by hand -->
<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>Naive Bayes classifier for texts — textmodel_nb • quanteda</title>

<!-- jquery -->
<script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script>
<!-- Bootstrap -->
<link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/readable/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">

<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<!-- Font Awesome icons -->
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">


<!-- pkgdown -->
<link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script>
<script src="../pkgdown.js"></script>
  <link href="../extra.css" rel="stylesheet">
  <script src="../extra.js"></script>
<!-- mathjax -->
<script src='https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>

<!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->


<!-- Google analytics -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-144616-24', 'auto');
  ga('send', 'pageview');

</script>

  </head>

  <body>
    <div class="container template-reference-topic">
      <header>
      <div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="../index.html">Quanteda</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Quick Start
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/pkgdown/quickstart.html">Quick Start Guide</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/quickstart_cn.html">快速入门指南</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/quickstart_ja.html">クイック・スタートガイド</a>
    </li>
  </ul>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/pkgdown/comparison.html">Features</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Examples
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/pkgdown/examples/phrase.html">Multi-word expressions</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/plotting.html">Textual data visualization</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/lsa.html">Latent Semantic Analysis (LSA)</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/chinese.html">Chinese text analysis</a>
    </li>
    <li>
      <a href="../articles/pkgdown/examples/twitter.html">Social media analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Replication
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="../articles/pkgdown/replication/digital-humanities.html">Text Analysis with R for Students of Literature</a>
    </li>
    <li>
      <a href="../articles/pkgdown/replication/text2vec.html">Word embedding (word2vec)</a>
    </li>
  </ul>
</li>
<li>
  <a href="../articles/design.html">Design</a>
</li>
      </ul>
      
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/kbenoit/quanteda">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      
      </header>

      <div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Naive Bayes classifier for texts</h1>
    </div>

    
    <p>Fit a multinomial or Bernoulli Naive Bayes model, given a dfm and some
training labels.</p>
    

    <pre class="usage"><span class='fu'>textmodel_nb</span>(<span class='no'>x</span>, <span class='no'>y</span>, <span class='kw'>smooth</span> <span class='kw'>=</span> <span class='fl'>1</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"uniform"</span>, <span class='st'>"docfreq"</span>, <span class='st'>"termfreq"</span>),
  <span class='kw'>distribution</span> <span class='kw'>=</span> <span class='fu'>c</span>(<span class='st'>"multinomial"</span>, <span class='st'>"Bernoulli"</span>), <span class='no'>...</span>)</pre>
    
    <h2 class="hasAnchor" id="arguments"><a class="anchor" href="#arguments"></a> Arguments</h2>
    <table class="ref-arguments">
    <colgroup><col class="name" /><col class="desc" /></colgroup>
    <tr>
      <th>x</th>
      <td><p>the <a href='dfm.html'>dfm</a> on which the model will be fit.  Does not need to contain 
only the training documents.</p></td>
    </tr>
    <tr>
      <th>y</th>
      <td><p>vector of training labels associated with each document identified 
in <code>train</code>.  (These will be converted to factors if not already 
factors.)</p></td>
    </tr>
    <tr>
      <th>smooth</th>
      <td><p>smoothing parameter for feature counts by class</p></td>
    </tr>
    <tr>
      <th>prior</th>
      <td><p>prior distribution on texts; one of <code>"uniform"</code>,
<code>"docfreq"</code>, or <code>"termfreq"</code>.  See Prior Distributions below.</p></td>
    </tr>
    <tr>
      <th>distribution</th>
      <td><p>count model for text features, can be <code>multinomial</code> 
or <code>Bernoulli</code>.  To fit a "binary multinomial" model, first convert the 
dfm to a binary matrix using <code><a href='tf.html'>tf</a>(x, "boolean")</code>.</p></td>
    </tr>
    <tr>
      <th>...</th>
      <td><p>more arguments passed through</p></td>
    </tr>
    </table>
    
    <h2 class="hasAnchor" id="value"><a class="anchor" href="#value"></a>Value</h2>

    <p>A list of return values, consisting of (where \(I\) is the total
  number of documents, \(J\) is the total number of features, and \(k\)
  is the total number of training classes):</p>
<dt>call</dt><dd><p>original function call</p></dd>

<dt>PwGc</dt><dd><p>\(k \times J\); probability of the word given the class (empirical 
  likelihood)</p></dd>

<dt>Pc</dt><dd><p>\(k\)-length named numeric vector of class prior probabilities</p></dd>

<dt>PcGw</dt><dd><p>\(k \times J\); posterior class probability given the word</p></dd>

<dt>Pw</dt><dd><p>\(J \times 1\); baseline probability of the word</p></dd>

<dt>data</dt><dd><p>list consisting of the \(I \times J\) training dfm
  <code>x</code>, and the \(I\)-length <code>y</code> training class vector</p></dd>

<dt>distribution</dt><dd><p>the distribution argument</p></dd>

<dt>prior</dt><dd><p>the prior argument</p></dd>

<dt>smooth</dt><dd><p>the value of the smoothing parameter</p></dd>

    
    <h2 class="hasAnchor" id="predict-methods"><a class="anchor" href="#predict-methods"></a>Predict Methods</h2>

    <p>A <code>predict</code> method is also available for a 
  fitted Naive Bayes object, see <code><a href='predict.textmodel.html'>predict.textmodel_nb_fitted</a></code>.</p>
    
    <h2 class="hasAnchor" id="prior-distributions"><a class="anchor" href="#prior-distributions"></a>Prior distributions</h2>

    
    <p>Prior distributions refer to the prior probabilities assigned to the training
classes, and the choice of prior distribution affects the calculation of the
fitted probabilities.  The default is uniform priors, which sets the
unconditional probability of observing the one class to be the same as
observing any other class.</p>
<p>"Document frequency" means that the class priors will be taken from the
relative proportions of the class documents used in the training set.  This
approach is so common that it is assumed in many examples, such as the worked
example from Manning, Raghavan, and Schütze (2008) below.  It is not the
default in <span class="pkg">quanteda</span>, however, since there may be nothing informative in
the relative numbers of documents used to train a classifier other than the
relative availability of the documents.  When training classes are balanced
in their number of documents (usually advisable), however, then the
empirically computed "docfreq" would be equivalent to "uniform" priors.</p>
<p>Setting <code>prior</code> to "termfreq" makes the priors equal to the proportions
of total feature counts found in the grouped documents in each training
class, so that the classes with the largest number of features are assigned
the largest priors. If the total count of features in each training class was
the same, then "uniform" and "termfreq" would be the same.</p>
    
    <h2 class="hasAnchor" id="references"><a class="anchor" href="#references"></a>References</h2>

    <p>Manning, C. D., Raghavan, P., &amp; Schütze, H. (2008). Introduction
  to Information Retrieval. Cambridge University Press.
  <a href='https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf'>https://nlp.stanford.edu/IR-book/pdf/irbookonlinereading.pdf</a></p>  
<p>Jurafsky, Daniel and James H. Martin. (2016) <em>Speech and Language Processing.</em>  Draft of November 7, 2016.
  <a href='https://web.stanford.edu/~jurafsky/slp3/6.pdf'>https://web.stanford.edu/~jurafsky/slp3/6.pdf</a></p>
    

    <h2 class="hasAnchor" id="examples"><a class="anchor" href="#examples"></a>Examples</h2>
    <pre class="examples"><div class='input'><span class='co'>## Example from 13.1 of _An Introduction to Information Retrieval_</span>
<span class='no'>txt</span> <span class='kw'>&lt;-</span> <span class='fu'>c</span>(<span class='kw'>d1</span> <span class='kw'>=</span> <span class='st'>"Chinese Beijing Chinese"</span>,
         <span class='kw'>d2</span> <span class='kw'>=</span> <span class='st'>"Chinese Chinese Shanghai"</span>,
         <span class='kw'>d3</span> <span class='kw'>=</span> <span class='st'>"Chinese Macao"</span>,
         <span class='kw'>d4</span> <span class='kw'>=</span> <span class='st'>"Tokyo Japan Chinese"</span>,
         <span class='kw'>d5</span> <span class='kw'>=</span> <span class='st'>"Chinese Chinese Chinese Tokyo Japan"</span>)
<span class='no'>trainingset</span> <span class='kw'>&lt;-</span> <span class='fu'><a href='dfm.html'>dfm</a></span>(<span class='no'>txt</span>, <span class='kw'>tolower</span> <span class='kw'>=</span> <span class='fl'>FALSE</span>)
<span class='no'>trainingclass</span> <span class='kw'>&lt;-</span> <span class='fu'>factor</span>(<span class='fu'>c</span>(<span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"Y"</span>, <span class='st'>"N"</span>, <span class='fl'>NA</span>), <span class='kw'>ordered</span> <span class='kw'>=</span> <span class='fl'>TRUE</span>)

<span class='co'>## replicate IIR p261 prediction for test set (document 5)</span>
(<span class='no'>nb.p261</span> <span class='kw'>&lt;-</span> <span class='fu'>textmodel_nb</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"docfreq"</span>))</div><div class='output co'>#&gt; Fitted Naive Bayes model:
#&gt; Call:
#&gt; 	textmodel_nb.dfm(x = trainingset, y = trainingclass, prior = "docfreq")
#&gt; 
#&gt; 
#&gt; Training classes and priors:
#&gt;    Y    N 
#&gt; 0.75 0.25 
#&gt; 
#&gt; 		  Likelihoods:		Class Posteriors:
#&gt;                   Y         N         Y         N
#&gt; Chinese  0.42857143 0.2222222 0.8526316 0.1473684
#&gt; Beijing  0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Shanghai 0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Macao    0.14285714 0.1111111 0.7941176 0.2058824
#&gt; Tokyo    0.07142857 0.2222222 0.4909091 0.5090909
#&gt; Japan    0.07142857 0.2222222 0.4909091 0.5090909
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='no'>nb.p261</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>trainingset</span>[<span class='fl'>5</span>, ])</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;       lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d5 -8.10769 -8.906681    0.6898 0.3102         Y
#&gt; </div><div class='input'>
<span class='co'># contrast with other priors</span>
<span class='fu'>predict</span>(<span class='fu'>textmodel_nb</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"uniform"</span>))</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d1 -4.333653 -5.898527    0.8271 0.1729         Y
#&gt; d2 -4.333653 -5.898527    0.8271 0.1729         Y
#&gt; d3 -3.486355 -4.394449    0.7126 0.2874         Y
#&gt; d4 -6.818560 -5.205379    0.1661 0.8339         N
#&gt; d5 -8.513155 -8.213534    0.4257 0.5743         N
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='fu'>textmodel_nb</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"termfreq"</span>))</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d1 -3.958960 -6.504662    0.9273 0.0727         Y
#&gt; d2 -3.958960 -6.504662    0.9273 0.0727         Y
#&gt; d3 -3.111662 -5.000585    0.8686 0.1314         Y
#&gt; d4 -6.443866 -5.811515    0.3470 0.6530         N
#&gt; d5 -8.138462 -8.819670    0.6640 0.3360         Y
#&gt; </div><div class='input'>
<span class='co'>## replicate IIR p264 Bernoulli Naive Bayes</span>
(<span class='no'>nb.p261.bern</span> <span class='kw'>&lt;-</span> <span class='fu'>textmodel_nb</span>(<span class='no'>trainingset</span>, <span class='no'>trainingclass</span>, <span class='kw'>distribution</span> <span class='kw'>=</span> <span class='st'>"Bernoulli"</span>,
                              <span class='kw'>prior</span> <span class='kw'>=</span> <span class='st'>"docfreq"</span>))</div><div class='output co'>#&gt; Fitted Naive Bayes model:
#&gt; Call:
#&gt; 	textmodel_nb.dfm(x = trainingset, y = trainingclass, prior = "docfreq", 
#&gt;     distribution = "Bernoulli")
#&gt; 
#&gt; 
#&gt; Training classes and priors:
#&gt;    Y    N 
#&gt; 0.75 0.25 
#&gt; 
#&gt; 		  Likelihoods:		Class Posteriors:
#&gt;            Y         N         Y         N
#&gt; Chinese  0.8 0.6666667 0.7826087 0.2173913
#&gt; Beijing  0.4 0.3333333 0.7826087 0.2173913
#&gt; Shanghai 0.4 0.3333333 0.7826087 0.2173913
#&gt; Macao    0.4 0.3333333 0.7826087 0.2173913
#&gt; Tokyo    0.2 0.6666667 0.4736842 0.5263158
#&gt; Japan    0.2 0.6666667 0.4736842 0.5263158
#&gt; </div><div class='input'><span class='fu'>predict</span>(<span class='no'>nb.p261.bern</span>, <span class='kw'>newdata</span> <span class='kw'>=</span> <span class='no'>trainingset</span>[<span class='fl'>5</span>, ])</div><div class='output co'>#&gt; Predicted textmodel of type: Naive Bayes
#&gt; 
#&gt;        lp(Y)     lp(N)     Pr(Y)  Pr(N) Predicted
#&gt; d5 -5.262178 -3.819085    0.1911 0.8089         N
#&gt; </div></pre>
  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <h2>Contents</h2>
    <ul class="nav nav-pills nav-stacked">
      <li><a href="#arguments">Arguments</a></li>
      
      <li><a href="#value">Value</a></li>

      <li><a href="#predict-methods">Predict Methods</a></li>

      <li><a href="#prior-distributions">Prior distributions</a></li>

      <li><a href="#references">References</a></li>
      
      <li><a href="#examples">Examples</a></li>
    </ul>

    <h2>Author</h2>
    
Kenneth Benoit

  </div>
</div>

      <footer>
      <div class="copyright">
  <p>Developed by Kenneth Benoit.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://hadley.github.io/pkgdown/">pkgdown</a>.</p>
</div>

      </footer>
   </div>

  </body>
</html>
